{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns: ['Gender', 'Working Professional or Student', 'Profession', 'Sleep Duration', 'Dietary Habits', 'Degree', 'Have you ever had suicidal thoughts ?', 'Family History of Mental Illness', 'city_type']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vg/fc9fvw412jz1fccwfsvm9d240000gp/T/ipykernel_11874/3441403676.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  working_professional_data.fillna(working_professional_data.mean(), inplace=True)\n",
      "/var/folders/vg/fc9fvw412jz1fccwfsvm9d240000gp/T/ipykernel_11874/3441403676.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  student_data.fillna(student_data.mean(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import StackingClassifier, VotingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Read train data\n",
    "train = pd.read_csv(\"train_new.csv\")\n",
    "\n",
    "# Drop unnecessary columns\n",
    "train = train.drop(['id', 'Name', 'City'], axis=1)\n",
    "\n",
    "# Detect categorical columns\n",
    "categorical_columns = train.select_dtypes(include=['object']).columns.tolist()\n",
    "print(\"Categorical columns:\", categorical_columns)\n",
    "\n",
    "# Label Encoding for categorical columns\n",
    "label_encoders = {}\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    train[col] = le.fit_transform(train[col].astype(str))  # Convert to string to handle any unexpected types\n",
    "    label_encoders[col] = le  # Save the encoder for potential inverse transformation\n",
    "\n",
    "# Split data based on 'Working Professional or Student' column\n",
    "working_professional_data = train[train['Working Professional or Student'] == 1]  # Assuming 1 = Working Professional\n",
    "student_data = train[train['Working Professional or Student'] == 0]  # Assuming 0 = Student\n",
    "\n",
    "working_professional_data.fillna(working_professional_data.mean(), inplace=True)\n",
    "student_data.fillna(student_data.mean(), inplace=True)\n",
    "\n",
    "# Drop the splitting column from feature sets\n",
    "X_wp = working_professional_data.drop(['Depression', 'Working Professional or Student','Academic Pressure','CGPA','Study Satisfaction'], axis=1)\n",
    "y_wp = working_professional_data['Depression']\n",
    "\n",
    "X_student = student_data.drop(['Depression', 'Working Professional or Student','Profession','Work Pressure','Job Satisfaction'], axis=1)\n",
    "y_student = student_data['Depression']\n",
    "\n",
    "# Split train and test for each group\n",
    "X_train_wp, X_test_wp, y_train_wp, y_test_wp = train_test_split(X_wp, y_wp, test_size=0.2, random_state=42, stratify=y_wp)\n",
    "X_train_student, X_test_student, y_train_student, y_test_student = train_test_split(X_student, y_student, test_size=0.2, random_state=42, stratify=y_student)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_student = scaler.fit_transform(X_train_student)\n",
    "X_test_student = scaler.transform(X_test_student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90080, 22520)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_wp), len(X_test_wp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training stacking ensemble for WP...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/SawaphobChavana/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/SawaphobChavana/Library/Python/3.9/lib/python/site-packages/xgboost/core.py:158: UserWarning: [16:49:51] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 7374, number of negative: 82706\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001318 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 184\n",
      "[LightGBM] [Info] Number of data points in the train set: 90080, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/SawaphobChavana/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/SawaphobChavana/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/SawaphobChavana/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/SawaphobChavana/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/SawaphobChavana/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 5899, number of negative: 66165\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 5900, number of negative: 66164\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003782 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 184\n",
      "[LightGBM] [Info] Number of data points in the train set: 72064, number of used features: 14\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000950 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 184\n",
      "[LightGBM] [Info] Number of data points in the train set: 72064, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 5899, number of negative: 66165\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 5899, number of negative: 66165\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001444 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 184\n",
      "[LightGBM] [Info] Number of data points in the train set: 72064, number of used features: 14\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003930 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Total Bins 184\n",
      "[LightGBM] [Info] Number of data points in the train set: 72064, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 5899, number of negative: 66165\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005373 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 184\n",
      "[LightGBM] [Info] Number of data points in the train set: 72064, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16:50:23] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "[16:50:23] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "[16:50:23] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "[16:50:23] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "[16:50:23] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Ensemble Accuracy for WP: 0.9594\n",
      "\n",
      "Training stacking ensemble for Student...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/SawaphobChavana/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/SawaphobChavana/Library/Python/3.9/lib/python/site-packages/xgboost/core.py:158: UserWarning: [16:51:35] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 13040, number of negative: 9228\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001865 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 383\n",
      "[LightGBM] [Info] Number of data points in the train set: 22268, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/SawaphobChavana/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/SawaphobChavana/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/SawaphobChavana/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/SawaphobChavana/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/SawaphobChavana/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 10432, number of negative: 7382\n",
      "[LightGBM] [Info] Number of positive: 10432, number of negative: 7382\n",
      "[LightGBM] [Info] Number of positive: 10432, number of negative: 7382\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004600 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 381\n",
      "[LightGBM] [Info] Number of data points in the train set: 17814, number of used features: 14\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002809 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 382\n",
      "[LightGBM] [Info] Number of positive: 10432, number of negative: 7383\n",
      "[LightGBM] [Info] Number of data points in the train set: 17814, number of used features: 14\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003308 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 381\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Number of data points in the train set: 17814, number of used features: 14\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 10432, number of negative: 7383\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001410 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 383\n",
      "[LightGBM] [Info] Number of data points in the train set: 17815, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001308 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 381\n",
      "[LightGBM] [Info] Number of data points in the train set: 17815, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16:51:46] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "[16:51:46] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "[16:51:46] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "[16:51:46] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "[16:51:46] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Ensemble Accuracy for student: 0.8452\n"
     ]
    }
   ],
   "source": [
    "# AdaBoost with Decision Tree\n",
    "dt = DecisionTreeClassifier(\n",
    "    max_depth=2,\n",
    "    min_samples_split=13,\n",
    "    min_samples_leaf=4,\n",
    "    class_weight='balanced',\n",
    "    random_state=40\n",
    ")\n",
    "\n",
    "ada_model = AdaBoostClassifier(\n",
    "    estimator=dt,\n",
    "    n_estimators=485,\n",
    "    learning_rate=0.026178,\n",
    "    algorithm='SAMME.R',\n",
    "    random_state=40\n",
    ")\n",
    "\n",
    "rf_model = RandomForestClassifier(\n",
    "    bootstrap=True,\n",
    "    class_weight='balanced',\n",
    "    criterion='gini',\n",
    "    max_depth=18,\n",
    "    max_features='sqrt',\n",
    "    max_leaf_nodes=170,\n",
    "    min_impurity_decrease=0.00055221171236024,\n",
    "    min_samples_leaf=3,\n",
    "    min_samples_split=18,\n",
    "    n_estimators=891,\n",
    "    n_jobs=-1,\n",
    "    random_state=40\n",
    ")\n",
    "# XGBoost parameters\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    colsample_bytree=0.6915192661966489,\n",
    "    gamma=0.038489954914396496,\n",
    "    learning_rate=0.0969254358741304,\n",
    "    max_bin=285,\n",
    "    max_depth=6,\n",
    "    min_child_weight=2,\n",
    "    n_estimators=781,\n",
    "    reg_alpha=1.616240759128834,\n",
    "    reg_lambda=1.266807513020847,\n",
    "    scale_pos_weight=0.061946902654867256,\n",
    "    subsample=0.9485842360750871,\n",
    "    tree_method='hist',\n",
    "    use_label_encoder=False,\n",
    ")\n",
    "\n",
    "# CatBoost parameters\n",
    "cat_model = cb.CatBoostClassifier(\n",
    "    subsample=0.6,\n",
    "    scale_pos_weight=0.061946902654867256,\n",
    "    random_strength=0.1,\n",
    "    min_child_samples=5,\n",
    "    learning_rate=0.03,\n",
    "    l2_leaf_reg=5,\n",
    "    iterations=300,\n",
    "    grow_policy=\"SymmetricTree\",\n",
    "    depth=6,\n",
    "    colsample_bylevel=1.0,\n",
    "    border_count=32,\n",
    "    bagging_temperature=0.8,\n",
    "    random_state=40,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "et_model = ExtraTreesClassifier(\n",
    "    n_estimators=700,\n",
    "    max_depth=None,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=1,\n",
    "    max_features='sqrt',\n",
    "    bootstrap=False,\n",
    "    random_state=40,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "optimized_gb_model = GradientBoostingClassifier(\n",
    "    learning_rate=0.04293117062858835,\n",
    "    max_depth=5,\n",
    "    max_features='sqrt',\n",
    "    min_samples_leaf=5,\n",
    "    min_samples_split=2,\n",
    "    n_estimators=918,\n",
    "    subsample=0.7077649335194086,\n",
    "    random_state=40\n",
    ")\n",
    "\n",
    "lgb_model = lgb.LGBMClassifier(\n",
    "    subsample=0.8,\n",
    "    reg_lambda=0,\n",
    "    reg_alpha=0.1,\n",
    "    num_leaves=31,\n",
    "    n_estimators=500,\n",
    "    min_child_samples=20,\n",
    "    max_depth=15,\n",
    "    learning_rate=0.15,\n",
    "    colsample_bytree=0.6,\n",
    "    class_weight='balanced',\n",
    "    random_state=40\n",
    ")\n",
    "\n",
    "# Create first-level ensemble\n",
    "base_estimators = [\n",
    "    ('xgb', xgb_model),\n",
    "    ('rf', rf_model),\n",
    "    ('ada', ada_model),\n",
    "    ('cat', cat_model),\n",
    "    ('et', et_model),\n",
    "    ('gb', optimized_gb_model),\n",
    "    ('lgb', lgb_model),\n",
    "]\n",
    "\n",
    "# Create stacking classifier\n",
    "stacking_wp = StackingClassifier(\n",
    "    estimators=base_estimators,\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "stacking_student = StackingClassifier(\n",
    "    estimators=base_estimators,\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"\\nTraining stacking ensemble for WP...\")\n",
    "stacking_wp.fit(X_train_wp, y_train_wp)\n",
    "y_pred = stacking_wp.predict(X_test_wp)\n",
    "accuracy = accuracy_score(y_test_wp, y_pred)\n",
    "print(f\"Stacking Ensemble Accuracy for WP: {accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nTraining stacking ensemble for Student...\")\n",
    "stacking_student.fit(X_train_student, y_train_student)\n",
    "y_pred = stacking_student.predict(X_test_student)\n",
    "accuracy = accuracy_score(y_test_student, y_pred)\n",
    "print(f\"Stacking Ensemble Accuracy for student: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vg/fc9fvw412jz1fccwfsvm9d240000gp/T/ipykernel_11874/642115030.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_wp.fillna(working_professional_data.mean(), inplace=True)\n",
      "/var/folders/vg/fc9fvw412jz1fccwfsvm9d240000gp/T/ipykernel_11874/642115030.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_student.fillna(student_data.mean(), inplace=True)\n",
      "/Users/SawaphobChavana/Library/Python/3.9/lib/python/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/SawaphobChavana/Library/Python/3.9/lib/python/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but AdaBoostClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/SawaphobChavana/Library/Python/3.9/lib/python/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but ExtraTreesClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/SawaphobChavana/Library/Python/3.9/lib/python/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but GradientBoostingClassifier was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# predict test\n",
    "test = pd.read_csv(\"test_new.csv\")\n",
    "test = test.drop(['Name', 'City'], axis=1)\n",
    "for col, le in label_encoders.items():\n",
    "    if col == 'id':\n",
    "        continue\n",
    "    test[col] = le.transform(test[col].astype(str))\n",
    "\n",
    "test_wp = test[test['Working Professional or Student'] == 1]\n",
    "test_student = test[test['Working Professional or Student'] == 0]\n",
    "\n",
    "test_wp_id = test_wp['id']\n",
    "test_student_id = test_student['id']\n",
    "\n",
    "test_wp.fillna(working_professional_data.mean(), inplace=True)\n",
    "test_student.fillna(student_data.mean(), inplace=True)\n",
    "\n",
    "X_test_wp = test_wp.drop(['id','Working Professional or Student','Academic Pressure','CGPA','Study Satisfaction'], axis=1)\n",
    "X_test_student = test_student.drop(['id','Working Professional or Student','Profession','Work Pressure','Job Satisfaction'], axis=1)\n",
    "\n",
    "\n",
    "y_pred_wp = stacking_wp.predict(X_test_wp)\n",
    "y_pred_student = stacking_student.predict(X_test_student)\n",
    "\n",
    "# join id and prediction using test_wp_id\n",
    "result_wp = pd.DataFrame()\n",
    "result_wp['id'] = test_wp_id\n",
    "result_wp['Depression'] = y_pred_wp\n",
    "\n",
    "result_student = pd.DataFrame()\n",
    "result_student['id'] = test_student_id\n",
    "result_student['Depression'] = y_pred_student\n",
    "\n",
    "result = pd.concat([result_wp, result_student])\n",
    "# sort by id\n",
    "result = result.sort_values(by='id')\n",
    "result.to_csv('submission_preprocess_stacking_separate.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
